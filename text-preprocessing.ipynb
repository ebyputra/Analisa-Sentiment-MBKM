{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18794c0e-cb18-486e-a2fa-af9a71ff891a",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa0e74e-0319-4b9b-912a-cd3b16fc75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6975ef8-9a1c-4aff-bb97-79a3f06f3539",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7d1abe8-025c-4619-8613-6cabf93756e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_crawling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "332b5f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kak kalau dibagian surat rekom, petingginya ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@holyjune00 aku lupa tapi KM 5 yang keterima g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@delviputriii12_ ga apa apa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@rama_aryo saya kemarin isi di Yogyakarta sela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kak Giman nih kalau lupa correct tidak sedang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>Cerita inspiratif dari saya peserta MSIB 4: ng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>min, sesekali saia masuk ke feed ini haha üòÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2111</th>\n",
       "      <td>waww ada kamu @fathiyyahazz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>Bisa aja capernya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>INFO BBHüî• KM UDAH CAIRRR MSIB KAPAN??</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2114 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     kak kalau dibagian surat rekom, petingginya ga...\n",
       "1     @holyjune00 aku lupa tapi KM 5 yang keterima g...\n",
       "2                           @delviputriii12_ ga apa apa\n",
       "3     @rama_aryo saya kemarin isi di Yogyakarta sela...\n",
       "4     Kak Giman nih kalau lupa correct tidak sedang ...\n",
       "...                                                 ...\n",
       "2109  Cerita inspiratif dari saya peserta MSIB 4: ng...\n",
       "2110        min, sesekali saia masuk ke feed ini haha üòÇ\n",
       "2111                        waww ada kamu @fathiyyahazz\n",
       "2112                                  Bisa aja capernya\n",
       "2113              INFO BBHüî• KM UDAH CAIRRR MSIB KAPAN??\n",
       "\n",
       "[2114 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65cf2d0-01c0-4d13-916c-d54df7ceb7ec",
   "metadata": {},
   "source": [
    "# Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d2d0f3-23fa-46bd-8ce4-3c9172ec29e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd779ce6-e884-4cea-b457-01dbaa9ab231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf38672f-8db1-4f10-8b61-71d34b45f273",
   "metadata": {},
   "source": [
    "# Deskripsi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67080a6b-9701-4b4d-8803-4f7edb6be04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>‚ù§Ô∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text\n",
       "count   2112\n",
       "unique  2073\n",
       "top       ‚ù§Ô∏è\n",
       "freq       6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297d75d-b42c-4823-b8be-0c5bcfaeb2fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fb10c8f-c35a-4d2e-8b47-1c67e50bb120",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d05b7b60-bc1f-4838-bcde-e6104e74cd24",
   "metadata": {},
   "source": [
    "## Remove punc,url,username,digits,whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ba11ea5-a861-4cd2-9018-617a3f439902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kak kalau dibagian surat rekom, petingginya ganyantumin pangkat dan golongannya boleh ga kak?',\n",
       " '@holyjune00 aku lupa tapi km 5 yang keterima ga sampe 50% dari total pendaftar',\n",
       " '@delviputriii12_ ga apa apa',\n",
       " '@rama_aryo saya kemarin isi di yogyakarta selain domisili',\n",
       " 'kak giman nih kalau lupa correct tidak sedang dapat beasiswa ü•∫ sya lupa kk solusinya ap ya',\n",
       " 'sedih rasanya km sudah masuk ke angkatan 6üò¢',\n",
       " '@delviputriii12_ https://t.me/pendaftarkm6',\n",
       " '@ayyu._.nabila gapapa masukin aja',\n",
       " '@rama_aryo iya kak masalahnya saya selain provinsi domisil itu diisi jauh dari domisili saya nama grup telenya aapa kak',\n",
       " '@yoppietri kak izin bertanya gimana kalo kita ikut organisasi tapi gk punya surat keterangan aktif organisasi, tapi kita ada sertifikat sertifikat pernah jadi panitia dan peserta di organisasi itu, boleh ndk kita taro sertifikat itu kak?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df.text.str.lower()\n",
    "text = list(df['text'].values)\n",
    "text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16f76858-6840-418c-b75f-694ccdb94edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kak kalau dibagian surat rekom petingginya ganyantumin pangkat dan golongannya boleh ga kak',\n",
       " 'aku lupa tapi km yang keterima ga sampe dari total pendaftar',\n",
       " 'ga apa apa',\n",
       " 'saya kemarin isi di yogyakarta selain domisili',\n",
       " 'kak giman nih kalau lupa correct tidak sedang dapat beasiswa sya lupa kk solusinya ap ya',\n",
       " 'sedih rasanya km sudah masuk ke angkatan',\n",
       " '',\n",
       " 'nabila gapapa masukin aja',\n",
       " 'iya kak masalahnya saya selain provinsi domisil itu diisi jauh dari domisili saya nama grup telenya aapa kak',\n",
       " 'kak izin bertanya gimana kalo kita ikut organisasi tapi gk punya surat keterangan aktif organisasi tapi kita ada sertifikat sertifikat pernah jadi panitia dan peserta di organisasi itu boleh ndk kita taro sertifikat itu kak']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punc_etc(text):\n",
    "    # remove url\n",
    "    text = [re.sub(r\"http\\S+\",\"\",t) for t in text]\n",
    "    \n",
    "    # remove username\n",
    "    text = [re.sub(\"([@#][A-Za-z0-9_]+)|(\\w+:\\/\\/\\S+)\",\"\", t) for t in text]\n",
    "    \n",
    "    # remove digits\n",
    "    text = [t.translate(str.maketrans('','',string.digits)).strip() for t in text]\n",
    "    \n",
    "    \n",
    "    # remove emoji\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text  = [emoji_pattern.sub(r'', t) for t in text]\n",
    "    text = [t.replace('ü•∫','') for t in text]\n",
    "    \n",
    "    #remove punc\n",
    "    text = [t.translate(str.maketrans('','',string.punctuation)).strip() for t in text]\n",
    "    \n",
    "    #remove whitespace\n",
    "    text = [t.replace('  ',' ').strip() for t in text]\n",
    "    \n",
    "    \n",
    "    return text\n",
    "text_remove_punc = remove_punc_etc(text)\n",
    "text_remove_punc[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13f962-3a96-4f01-a5e0-a32ed7a5a9b1",
   "metadata": {},
   "source": [
    "## Normalisasi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6388b3de-a5b2-451b-a0a5-168d50a1c72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kak kalau dibagian surat rekom petingginya ganyantumin pangkat dan golongannya boleh tidak kak',\n",
       " 'saya lupa tapi kamu yang keterima tidak sampai dari total pendaftar',\n",
       " 'tidak apa apa',\n",
       " 'saya kemarin isi di yogyakarta selain domisili',\n",
       " 'kak giman nih kalau lupa correct tidak sedang dapat beasiswa saya lupa kk solusinya ap ya',\n",
       " 'sedih rasanya kamu sudah masuk ke angkatan',\n",
       " '',\n",
       " 'nabila tidak apa-apa masukin saja',\n",
       " 'iya kak masalahnya saya selain provinsi domisil itu diisi jauh dari domisili saya nama grup telenya aapa kak',\n",
       " 'kak izin bertanya bagaimana kalau kita ikut organisasi tapi tidak punya surat keterangan aktif organisasi tapi kita ada sertifikat sertifikat pernah jadi panitia dan peserta di organisasi itu boleh tidak kita taro sertifikat itu kak']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = pd.read_csv('data_requirement/normalisasi_kata.csv')\n",
    "def normalisasi(text):\n",
    "    text = [t.split() for t in text]\n",
    "    text_ = []\n",
    "\n",
    "    for t in text:\n",
    "        c = []\n",
    "        for j in t:\n",
    "            if j in norm['singkat'].values:\n",
    "                hasil = ''.join(norm[norm['singkat'] == j]['hasil'])\n",
    "                c.append(hasil)\n",
    "            else:\n",
    "                c.append(j)\n",
    "        text_.append(c)\n",
    "    return [' '.join(t) for t in text_]\n",
    "text_norm = normalisasi(text_remove_punc)\n",
    "text_norm[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f83a967-2d98-4a63-b2df-f2c62854ffa4",
   "metadata": {},
   "source": [
    "## Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94490154-bf07-4c1c-9be9-52716e6768b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\By/nltk_data'\n    - 'C:\\\\Users\\\\By\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\By\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\By\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\By\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m [nltk\u001b[38;5;241m.\u001b[39mword_tokenize(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m----> 5\u001b[0m text_token \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_token[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m])\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mword_tokenize_wrapper\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize_wrapper\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     text \u001b[38;5;241m=\u001b[39m [nltk\u001b[38;5;241m.\u001b[39mword_tokenize(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize_wrapper\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     text \u001b[38;5;241m=\u001b[39m [\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\By/nltk_data'\n    - 'C:\\\\Users\\\\By\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\By\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\By\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\By\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "def word_tokenize_wrapper(text):\n",
    "    text = [nltk.word_tokenize(t) for t in text]\n",
    "    return text\n",
    "\n",
    "text_token = word_tokenize_wrapper(text_norm)\n",
    "print(text_token[0:3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4f4c32-33d8-4419-9de0-046ce2a3a085",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50efa9e8-518a-46e2-9736-8b74a0c564f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "----- sample -----\n",
      "{'kak': 'kak', 'kalau': 'kalau', 'dibagian': 'bagi', 'surat': 'surat', 'rekom': 'rekom', 'petingginya': 'petinggi', 'ganyantumin': 'ganyantumin', 'pangkat': 'pangkat', 'dan': 'dan', 'golongannya': 'golong', 'boleh': 'boleh', 'tidak': 'tidak', 'saya': 'saya', 'lupa': 'lupa', 'tapi': 'tapi', 'kamu': 'kamu', 'yang': 'yang', 'keterima': 'terima', 'sampai': 'sampai', 'dari': 'dari', 'total': 'total', 'pendaftar': 'daftar', 'apa': 'apa', 'kemarin': 'kemarin', 'isi': 'isi', 'di': 'di', 'yogyakarta': 'yogyakarta', 'selain': 'selain', 'domisili': 'domisili', 'giman': 'gim', 'nih': 'nih', 'correct': 'correct', 'sedang': 'sedang', 'dapat': 'dapat', 'beasiswa': 'beasiswa', 'kk': 'kk', 'solusinya': 'solusi', 'ap': 'ap', 'ya': 'ya', 'sedih': 'sedih', 'rasanya': 'rasa', 'sudah': 'sudah', 'masuk': 'masuk', 'ke': 'ke', 'angkatan': 'angkat', 'nabila': 'nabila', 'apa-apa': 'apa', 'masukin': 'masukin', 'saja': 'saja', 'iya': 'iya', 'masalahnya': 'masalah', 'provinsi': 'provinsi', 'domisil': 'domisil', 'itu': 'itu', 'diisi': 'isi', 'jauh': 'jauh', 'nama': 'nama', 'grup': 'grup', 'telenya': 'tele', 'aapa': 'aapa', 'izin': 'izin', 'bertanya': 'tanya', 'bagaimana': 'bagaimana', 'kita': 'kita', 'ikut': 'ikut', 'organisasi': 'organisasi', 'punya': 'punya', 'keterangan': 'terang', 'aktif': 'aktif', 'ada': 'ada', 'sertifikat': 'sertifikat', 'pernah': 'pernah', 'jadi': 'jadi', 'panitia': 'panitia', 'peserta': 'serta', 'taro': 'taro'}\n"
     ]
    }
   ],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in text_token[0:10]:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "print(len(term_dict))\n",
    "print('----- sample -----')\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "print(term_dict)\n",
    "\n",
    "# apply\n",
    "def stemming(text):\n",
    "    c  = []\n",
    "    for text in text_token:\n",
    "        t_ = []\n",
    "        for t in text:\n",
    "            t_.append(stemmer.stem(t))\n",
    "        c.append(t_)\n",
    "    hasil = []\n",
    "    for t in c:\n",
    "        hasil.append(' '.join(c for c in t ))\n",
    "    return hasil\n",
    "\n",
    "\n",
    "text_stemming = stemming(text_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7048f954-f7b1-4c0c-a2bd-50e97a864d05",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "141ce7e5-c421-4b03-9d1b-805705a35183",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_stemming' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m         text_\u001b[38;5;241m.\u001b[39mappend(c)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text_]\n\u001b[1;32m---> 13\u001b[0m text_stop \u001b[38;5;241m=\u001b[39m stopwords(\u001b[43mtext_stemming\u001b[49m)\n\u001b[0;32m     14\u001b[0m text_stop[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m10\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_stemming' is not defined"
     ]
    }
   ],
   "source": [
    "stopwords_idn = list(pd.read_csv('data_requirement/stopwords_idn.csv').values)\n",
    "stopwords_ = stopwords_idn+['up','wkwk','a',\"bismillah\",'min','ya','nya','kak']\n",
    "def stopwords(text):\n",
    "    text__ = [t.split() for t in text]\n",
    "    text_ = []\n",
    "    for t in text__:\n",
    "        c = []\n",
    "        for j in t:\n",
    "            if j not in stopwords_:\n",
    "                c.append(j)\n",
    "        text_.append(c)\n",
    "    return [' '.join(t) for t in text_]\n",
    "text_stop = stopwords(text_stemming)\n",
    "text_stop[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "186dd445-e846-4546-88ee-22f0924c3d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112\n",
      "2112\n",
      "2112\n",
      "2112\n",
      "2112\n",
      "2112\n"
     ]
    }
   ],
   "source": [
    "print(len(text_stemming))\n",
    "print(len(text_token))\n",
    "print(len(text_stop))\n",
    "print(len(text_norm))\n",
    "print(len(text_remove_punc))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cdd5cc-68ed-42a0-8068-aebde2137b5f",
   "metadata": {},
   "source": [
    "# Tahap Akhir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de245494-961f-42ae-afd7-a50d5c8ce82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_kotor</th>\n",
       "      <th>text_remove_punc</th>\n",
       "      <th>text_norm</th>\n",
       "      <th>token</th>\n",
       "      <th>text_stemming</th>\n",
       "      <th>text_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kak kalau dibagian surat rekom, petingginya ga...</td>\n",
       "      <td>kak kalau dibagian surat rekom petingginya gan...</td>\n",
       "      <td>kak kalau dibagian surat rekom petingginya gan...</td>\n",
       "      <td>[kak, kalau, dibagian, surat, rekom, petinggin...</td>\n",
       "      <td>kak kalau bagi surat rekom petinggi ganyantumi...</td>\n",
       "      <td>surat rekom petinggi ganyantumin pangkat golong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@holyjune00 aku lupa tapi km 5 yang keterima g...</td>\n",
       "      <td>aku lupa tapi km yang keterima ga sampe dari t...</td>\n",
       "      <td>saya lupa tapi kamu yang keterima tidak sampai...</td>\n",
       "      <td>[saya, lupa, tapi, kamu, yang, keterima, tidak...</td>\n",
       "      <td>saya lupa tapi kamu yang terima tidak sampai d...</td>\n",
       "      <td>lupa terima total daftar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@delviputriii12_ ga apa apa</td>\n",
       "      <td>ga apa apa</td>\n",
       "      <td>tidak apa apa</td>\n",
       "      <td>[tidak, apa, apa]</td>\n",
       "      <td>tidak apa apa</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@rama_aryo saya kemarin isi di yogyakarta sela...</td>\n",
       "      <td>saya kemarin isi di yogyakarta selain domisili</td>\n",
       "      <td>saya kemarin isi di yogyakarta selain domisili</td>\n",
       "      <td>[saya, kemarin, isi, di, yogyakarta, selain, d...</td>\n",
       "      <td>saya kemarin isi di yogyakarta selain domisili</td>\n",
       "      <td>kemarin isi yogyakarta domisili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kak giman nih kalau lupa correct tidak sedang ...</td>\n",
       "      <td>kak giman nih kalau lupa correct tidak sedang ...</td>\n",
       "      <td>kak giman nih kalau lupa correct tidak sedang ...</td>\n",
       "      <td>[kak, giman, nih, kalau, lupa, correct, tidak,...</td>\n",
       "      <td>kak gim nih kalau lupa correct tidak sedang da...</td>\n",
       "      <td>gim nih lupa correct beasiswa lupa kk solusi ap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>cerita inspiratif dari saya peserta msib 4: ng...</td>\n",
       "      <td>cerita inspiratif dari saya peserta msib ngiri...</td>\n",
       "      <td>cerita inspiratif dari saya peserta msib ngiri...</td>\n",
       "      <td>[cerita, inspiratif, dari, saya, peserta, msib...</td>\n",
       "      <td>cerita inspiratif dari saya serta msib ngirit ...</td>\n",
       "      <td>cerita inspiratif msib ngirit bbh turun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108</th>\n",
       "      <td>min, sesekali saia masuk ke feed ini haha üòÇ</td>\n",
       "      <td>min sesekali saia masuk ke feed ini haha</td>\n",
       "      <td>min sesekali saya masuk ke feed ini haha</td>\n",
       "      <td>[min, sesekali, saya, masuk, ke, feed, ini, haha]</td>\n",
       "      <td>min sekal saya masuk ke feed ini haha</td>\n",
       "      <td>sekal masuk feed haha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>waww ada kamu @fathiyyahazz</td>\n",
       "      <td>waww ada kamu</td>\n",
       "      <td>waww ada kamu</td>\n",
       "      <td>[waww, ada, kamu]</td>\n",
       "      <td>waww ada kamu</td>\n",
       "      <td>waww</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>bisa aja capernya</td>\n",
       "      <td>bisa aja capernya</td>\n",
       "      <td>bisa saja capernya</td>\n",
       "      <td>[bisa, saja, capernya]</td>\n",
       "      <td>bisa saja capernya</td>\n",
       "      <td>capernya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2111</th>\n",
       "      <td>info bbhüî• km udah cairrr msib kapan??</td>\n",
       "      <td>info bbh km udah cairrr msib kapan</td>\n",
       "      <td>informasi bbh kamu sudah cairrr msib kapan</td>\n",
       "      <td>[informasi, bbh, kamu, sudah, cairrr, msib, ka...</td>\n",
       "      <td>informasi bbh kamu sudah cairrr msib kapan</td>\n",
       "      <td>informasi bbh cairrr msib</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2112 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_kotor  \\\n",
       "0     kak kalau dibagian surat rekom, petingginya ga...   \n",
       "1     @holyjune00 aku lupa tapi km 5 yang keterima g...   \n",
       "2                           @delviputriii12_ ga apa apa   \n",
       "3     @rama_aryo saya kemarin isi di yogyakarta sela...   \n",
       "4     kak giman nih kalau lupa correct tidak sedang ...   \n",
       "...                                                 ...   \n",
       "2107  cerita inspiratif dari saya peserta msib 4: ng...   \n",
       "2108        min, sesekali saia masuk ke feed ini haha üòÇ   \n",
       "2109                        waww ada kamu @fathiyyahazz   \n",
       "2110                                  bisa aja capernya   \n",
       "2111              info bbhüî• km udah cairrr msib kapan??   \n",
       "\n",
       "                                       text_remove_punc  \\\n",
       "0     kak kalau dibagian surat rekom petingginya gan...   \n",
       "1     aku lupa tapi km yang keterima ga sampe dari t...   \n",
       "2                                            ga apa apa   \n",
       "3        saya kemarin isi di yogyakarta selain domisili   \n",
       "4     kak giman nih kalau lupa correct tidak sedang ...   \n",
       "...                                                 ...   \n",
       "2107  cerita inspiratif dari saya peserta msib ngiri...   \n",
       "2108           min sesekali saia masuk ke feed ini haha   \n",
       "2109                                      waww ada kamu   \n",
       "2110                                  bisa aja capernya   \n",
       "2111                 info bbh km udah cairrr msib kapan   \n",
       "\n",
       "                                              text_norm  \\\n",
       "0     kak kalau dibagian surat rekom petingginya gan...   \n",
       "1     saya lupa tapi kamu yang keterima tidak sampai...   \n",
       "2                                         tidak apa apa   \n",
       "3        saya kemarin isi di yogyakarta selain domisili   \n",
       "4     kak giman nih kalau lupa correct tidak sedang ...   \n",
       "...                                                 ...   \n",
       "2107  cerita inspiratif dari saya peserta msib ngiri...   \n",
       "2108           min sesekali saya masuk ke feed ini haha   \n",
       "2109                                      waww ada kamu   \n",
       "2110                                 bisa saja capernya   \n",
       "2111         informasi bbh kamu sudah cairrr msib kapan   \n",
       "\n",
       "                                                  token  \\\n",
       "0     [kak, kalau, dibagian, surat, rekom, petinggin...   \n",
       "1     [saya, lupa, tapi, kamu, yang, keterima, tidak...   \n",
       "2                                     [tidak, apa, apa]   \n",
       "3     [saya, kemarin, isi, di, yogyakarta, selain, d...   \n",
       "4     [kak, giman, nih, kalau, lupa, correct, tidak,...   \n",
       "...                                                 ...   \n",
       "2107  [cerita, inspiratif, dari, saya, peserta, msib...   \n",
       "2108  [min, sesekali, saya, masuk, ke, feed, ini, haha]   \n",
       "2109                                  [waww, ada, kamu]   \n",
       "2110                             [bisa, saja, capernya]   \n",
       "2111  [informasi, bbh, kamu, sudah, cairrr, msib, ka...   \n",
       "\n",
       "                                          text_stemming  \\\n",
       "0     kak kalau bagi surat rekom petinggi ganyantumi...   \n",
       "1     saya lupa tapi kamu yang terima tidak sampai d...   \n",
       "2                                         tidak apa apa   \n",
       "3        saya kemarin isi di yogyakarta selain domisili   \n",
       "4     kak gim nih kalau lupa correct tidak sedang da...   \n",
       "...                                                 ...   \n",
       "2107  cerita inspiratif dari saya serta msib ngirit ...   \n",
       "2108              min sekal saya masuk ke feed ini haha   \n",
       "2109                                      waww ada kamu   \n",
       "2110                                 bisa saja capernya   \n",
       "2111         informasi bbh kamu sudah cairrr msib kapan   \n",
       "\n",
       "                                            text_stop  \n",
       "0     surat rekom petinggi ganyantumin pangkat golong  \n",
       "1                            lupa terima total daftar  \n",
       "2                                                      \n",
       "3                     kemarin isi yogyakarta domisili  \n",
       "4     gim nih lupa correct beasiswa lupa kk solusi ap  \n",
       "...                                               ...  \n",
       "2107          cerita inspiratif msib ngirit bbh turun  \n",
       "2108                            sekal masuk feed haha  \n",
       "2109                                             waww  \n",
       "2110                                         capernya  \n",
       "2111                        informasi bbh cairrr msib  \n",
       "\n",
       "[2112 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'text_kotor':text,'text_remove_punc':text_remove_punc,'text_norm':text_norm,'token':text_token,'text_stemming':text_stemming,'text_stop':text_stop}\n",
    "df_ = pd.DataFrame(d)\n",
    "# df_ = pd.DataFrame(d)\n",
    "\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8080393-b831-402a-b4c0-d6d2719b36ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_kotor          0\n",
       "text_remove_punc    0\n",
       "text_norm           0\n",
       "token               0\n",
       "text_stemming       0\n",
       "text_stop           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aadf3c4-fd3e-407d-b70c-bba7ebae84c4",
   "metadata": {},
   "source": [
    "## Hapus Teks kosong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89b8af03-b021-4034-9b7e-e8a641dbee95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1874"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# while(\"\" in text_stop):\n",
    "while(\"\" in text_stop):\n",
    "    text_stop.remove(\"\")\n",
    "len(text_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf07631-7250-40e4-95f6-49c838ece480",
   "metadata": {},
   "source": [
    "## Kata yang sering muncul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8f727ab-715f-48b8-a23f-6760d37f578b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supported for TrueType fonts",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16288/2010536244.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mall_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m wordcloud = WordCloud(width = 800, height = 800,\n\u001b[0m\u001b[0;32m      3\u001b[0m                 \u001b[0mbackground_color\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'white'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 min_font_size = 10).generate(all_words)\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \"\"\"\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    619\u001b[0m         \"\"\"\n\u001b[0;32m    620\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[0mfont_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m                 self.generate_from_frequencies(dict(frequencies[:2]),\n\u001b[0m\u001b[0;32m    454\u001b[0m                                                max_font_size=self.height)\n\u001b[0;32m    455\u001b[0m                 \u001b[1;31m# find font sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    506\u001b[0m                     font, orientation=orientation)\n\u001b[0;32m    507\u001b[0m                 \u001b[1;31m# get size of resulting text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m                 \u001b[0mbox_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextbbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransposed_font\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"lt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m                 \u001b[1;31m# find possible places using integral image:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m                 result = occupancy.sample_position(box_size[3] + self.margin,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\ImageDraw.py\u001b[0m in \u001b[0;36mtextbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetfont\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreeTypeFont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Only supported for TrueType fonts\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"RGBA\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0membedded_color\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfontmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m         bbox = font.getbbox(\n",
      "\u001b[1;31mValueError\u001b[0m: Only supported for TrueType fonts"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join([t for t in text_stop])\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate(all_words)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52c006-16d6-426b-abfa-89bacb53412a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e270242-e177-4812-9413-c2431c6d2b8a",
   "metadata": {},
   "source": [
    "setelah melakukan text preprocessing data yang sudah bersih di export guna melakukan tahap selanjutnya yaitu labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e88726-6015-4f6d-9e40-d6dd593f2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text_clean':text_stop})\n",
    "df.to_csv('text_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78792ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
